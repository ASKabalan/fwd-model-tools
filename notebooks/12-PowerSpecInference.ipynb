{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Power Spectrum Inference\n",
    "\n",
    "This notebook demonstrates **power-spectrum-level** Bayesian inference of cosmological\n",
    "parameters from convergence maps using the Knox-formula likelihood.\n",
    "\n",
    "Compared to the full-field approach (notebook 11), this is computationally cheaper\n",
    "because the data compression step (maps → C_ell) is done once up-front:\n",
    "\n",
    "1. Load observed convergence maps from a Parquet catalog\n",
    "2. Compress to all auto- (and optionally cross-) angular power spectra\n",
    "3. Build the NumPyro power-spectrum model with the Knox-formula likelihood\n",
    "4. Sample the posterior with MCMC (`batched_sampling`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.97\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax_cosmo as jc\n",
    "import jax_fli as jfli\n",
    "import numpy as np\n",
    "from numpyro.handlers import condition\n",
    "\n",
    "print(f\"Number of devices: {jax.device_count()}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "jax.config.update(\"jax_enable_x64\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Load Catalog\n",
    "\n",
    "Set `CATALOG_PATH` to a Parquet file produced by a previous simulation run.\n",
    "The catalog contains convergence maps for each redshift bin as a batched\n",
    "`SphericalDensity` (or `FlatDensity`) field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_PATH = \"output/fields/born_catalog.parquet\"  # <- update to your path\n",
    "\n",
    "catalog = jfli.io.Catalog.from_parquet(CATALOG_PATH)\n",
    "print(f\"Catalog entries: {len(catalog)}\")\n",
    "\n",
    "# The catalog field is a batched SphericalDensity with shape (n_bins, npix)\n",
    "kappa_field = catalog.field[0]\n",
    "print(f\"Kappa field type:  {type(kappa_field).__name__}\")\n",
    "print(f\"Kappa field shape: {kappa_field.array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Compute Observed C_ell\n",
    "\n",
    "`cross_angular_cl()` computes all K = B*(B+1)/2 auto- and cross-spectra for a\n",
    "batched field with B bins in a single healpy call.  The returned `PowerSpectrum`\n",
    "stores the ell-array in `.wavenumber` and all spectra in `.array` with shape\n",
    "`(K, n_ell)`, ordered `(0,0), (0,1), …, (B-1,B-1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For spherical maps use lmax; for flat maps use ell_edges instead\n",
    "LMAX = 512\n",
    "\n",
    "ps = kappa_field.cross_angular_cl(lmax=LMAX)\n",
    "\n",
    "print(f\"wavenumber shape: {ps.wavenumber.shape}\")\n",
    "print(f\"spectra shape:    {ps.array.shape}\")\n",
    "print(f\"ell range: [{ps.wavenumber[0]:.0f}, {ps.wavenumber[-1]:.0f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Build Configuration\n",
    "\n",
    "We pass `ells=ps.wavenumber` so the theory model evaluates at exactly the\n",
    "same multipoles as the observed spectra. `f_sky` corrects the Knox formula\n",
    "for partial sky coverage; set it to the observed sky fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_sources = jfli.io.get_stage3_nz_shear()\n",
    "\n",
    "priors = {\n",
    "    \"Omega_c\": jfli.infer.dist.PreconditionnedUniform(0.1, 0.5),\n",
    "    \"sigma8\": jfli.infer.dist.PreconditionnedUniform(0.6, 1.0),\n",
    "}\n",
    "\n",
    "config = jfli.ppl.Configurations(\n",
    "    mesh_size=(64, 64, 64),  # not used by power-spec model but required by dataclass\n",
    "    box_size=(500.0, 500.0, 500.0),\n",
    "    nside=kappa_field.nside,\n",
    "    nz_shear=nz_sources,\n",
    "    fiducial_cosmology=jc.Planck18,\n",
    "    sigma_e=0.26,\n",
    "    priors=priors,\n",
    "    geometry=\"spherical\",\n",
    "    ells=ps.wavenumber,\n",
    "    f_sky=1.0,  # update to the actual observed sky fraction\n",
    ")\n",
    "\n",
    "print(f\"nside:     {config.nside}\")\n",
    "print(f\"f_sky:     {config.f_sky}\")\n",
    "print(f\"n_ells:    {len(config.ells)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Condition on Observed Data\n",
    "\n",
    "`numpyro.handlers.condition` replaces the `C_ell_*` sample sites with the\n",
    "observed spectra, turning the joint model into a likelihood for the\n",
    "cosmological parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = jfli.ppl.powerspec_probmodel(config)\n",
    "obs = {\"c_ell\": ps.array.flatten()}\n",
    "conditioned = condition(model, data=obs)\n",
    "\n",
    "# Starting point for HMC: fiducial cosmology values\n",
    "fiducial = jc.Planck18()\n",
    "init_params = {\n",
    "    \"Omega_c\": jnp.array(fiducial.Omega_c),\n",
    "    \"sigma8\": jnp.array(fiducial.sigma8),\n",
    "}\n",
    "\n",
    "print(\"Init params:\")\n",
    "for k, v in init_params.items():\n",
    "    print(f\"  {k}: {float(v):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Define Save Callback\n",
    "\n",
    "For power-spectrum inference the samples are small (just cosmological parameters),\n",
    "so we save them as a simple `.npz` file instead of a Parquet catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Run MCMC Sampling\n",
    "\n",
    "`batched_sampling` runs NUTS in sequential batches, checkpointing state after\n",
    "each batch so a long run can be interrupted and resumed.\n",
    "\n",
    "For production use, increase `num_warmup`, `num_samples`, and `batch_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"output/ps_inference\"\n",
    "sample_key = jax.random.PRNGKey(42)\n",
    "\n",
    "jfli.infer.batched_sampling(\n",
    "    conditioned,\n",
    "    init_params=init_params,\n",
    "    path=OUTPUT_PATH,\n",
    "    rng_key=sample_key,\n",
    "    num_warmup=100,\n",
    "    num_samples=500,\n",
    "    batch_count=10,\n",
    "    sampler=\"NUTS\",\n",
    "    backend=\"blackjax\",\n",
    "    progress_bar=True,\n",
    "    save_callback=jfli.infer.sample2catalog(config),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Load and Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "npz_files = sorted(glob.glob(f\"{OUTPUT_PATH}/samples_*.npz\"))\n",
    "print(f\"Found {len(npz_files)} batch file(s): {npz_files}\")\n",
    "\n",
    "all_samples = {}\n",
    "for f in npz_files:\n",
    "    data = np.load(f)\n",
    "    for k in data.files:\n",
    "        all_samples.setdefault(k, []).append(data[k])\n",
    "\n",
    "# Concatenate batches along the sample axis\n",
    "all_samples = {k: np.concatenate(v, axis=0) for k, v in all_samples.items()}\n",
    "\n",
    "print(\"\\nParameter shapes and posterior means:\")\n",
    "for k, v in all_samples.items():\n",
    "    if v.ndim >= 1 and np.issubdtype(v.dtype, np.floating):\n",
    "        print(f\"  {k}: shape={v.shape}  mean={v.mean():.4f}  std={v.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
